{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a050c79",
   "metadata": {},
   "source": [
    "## Web Scraping Approach\n",
    "\n",
    "A web scraping process aimed at collecting news articles from an online platform is performed. 3 libraries were used to carry out our web scraping process.\n",
    "\n",
    "- `requests`: Retrieving web page content.(Python Software Foundation, n.d.)\n",
    "- `BeautifulSoup`: Parsing and navigating the HTML structure of these pages for data extraction.(Mitchell & Richardson, n.d.)\n",
    "- `pandas`: To structure the scraped information into a format ready for further analysis. (McKinney, n.d.)\n",
    "\n",
    "Using the requests and BeautifulSoup libraries (Python Software Foundation 2022; Crummy 2021), text material from Project Gutenberg ebooks is scraped from the web. Based on random ebook IDs, we develop a method to scrape text excerpts and ebook titles. For analysis, the data that was scraped is kept in a pandas DataFrame. Until the desired number of ebooks is attained, the procedure is repeated. After that, the DataFrame is stored for later use in a pickle file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710b6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132cb827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected articles: 1/150\n",
      "Collected articles: 2/150\n",
      "Collected articles: 3/150\n",
      "Collected articles: 4/150\n",
      "Collected articles: 5/150\n",
      "Collected articles: 6/150\n",
      "Collected articles: 7/150\n",
      "Collected articles: 8/150\n",
      "Collected articles: 9/150\n",
      "Collected articles: 10/150\n",
      "Collected articles: 11/150\n",
      "Collected articles: 12/150\n",
      "Collected articles: 13/150\n",
      "Collected articles: 14/150\n",
      "Collected articles: 15/150\n",
      "Collected articles: 16/150\n",
      "Collected articles: 17/150\n",
      "Collected articles: 18/150\n",
      "Collected articles: 19/150\n",
      "Collected articles: 20/150\n",
      "Collected articles: 21/150\n",
      "Collected articles: 22/150\n",
      "Collected articles: 23/150\n",
      "Collected articles: 24/150\n",
      "Collected articles: 25/150\n",
      "Collected articles: 26/150\n",
      "Collected articles: 27/150\n",
      "Collected articles: 28/150\n",
      "Collected articles: 29/150\n",
      "Collected articles: 30/150\n",
      "Collected articles: 31/150\n",
      "Collected articles: 32/150\n",
      "Collected articles: 33/150\n",
      "Collected articles: 34/150\n",
      "Collected articles: 35/150\n",
      "Collected articles: 36/150\n",
      "Collected articles: 37/150\n",
      "Collected articles: 38/150\n",
      "Collected articles: 39/150\n",
      "Collected articles: 40/150\n",
      "Collected articles: 41/150\n",
      "Collected articles: 42/150\n",
      "Collected articles: 43/150\n",
      "Collected articles: 44/150\n",
      "Collected articles: 45/150\n",
      "Collected articles: 46/150\n",
      "Collected articles: 47/150\n",
      "Collected articles: 48/150\n",
      "Collected articles: 49/150\n",
      "Collected articles: 50/150\n",
      "Collected articles: 51/150\n",
      "Collected articles: 52/150\n",
      "Collected articles: 53/150\n",
      "Collected articles: 54/150\n",
      "Collected articles: 55/150\n",
      "Collected articles: 56/150\n",
      "Collected articles: 57/150\n",
      "Collected articles: 58/150\n",
      "Collected articles: 59/150\n",
      "Collected articles: 60/150\n",
      "Collected articles: 61/150\n",
      "Collected articles: 62/150\n",
      "Collected articles: 63/150\n",
      "Collected articles: 64/150\n",
      "Collected articles: 65/150\n",
      "Collected articles: 66/150\n",
      "Collected articles: 67/150\n",
      "Collected articles: 68/150\n",
      "Collected articles: 69/150\n",
      "Collected articles: 70/150\n",
      "Collected articles: 71/150\n",
      "Collected articles: 72/150\n",
      "Collected articles: 73/150\n",
      "Collected articles: 74/150\n",
      "Collected articles: 75/150\n",
      "Collected articles: 76/150\n",
      "Collected articles: 77/150\n",
      "Collected articles: 78/150\n",
      "Collected articles: 79/150\n",
      "Collected articles: 80/150\n",
      "Collected articles: 81/150\n",
      "Collected articles: 82/150\n",
      "Collected articles: 83/150\n",
      "Collected articles: 84/150\n",
      "Collected articles: 85/150\n",
      "Collected articles: 86/150\n",
      "Collected articles: 87/150\n",
      "Collected articles: 88/150\n",
      "Collected articles: 89/150\n",
      "Collected articles: 90/150\n",
      "Collected articles: 91/150\n",
      "Collected articles: 92/150\n",
      "Collected articles: 93/150\n",
      "Collected articles: 94/150\n",
      "Collected articles: 95/150\n",
      "Collected articles: 96/150\n",
      "Collected articles: 97/150\n",
      "Collected articles: 98/150\n",
      "Collected articles: 99/150\n",
      "Collected articles: 100/150\n",
      "Collected articles: 101/150\n",
      "Collected articles: 102/150\n",
      "Collected articles: 103/150\n",
      "Collected articles: 104/150\n",
      "Collected articles: 105/150\n",
      "Collected articles: 106/150\n",
      "Collected articles: 107/150\n",
      "Collected articles: 108/150\n",
      "Collected articles: 109/150\n",
      "Collected articles: 110/150\n",
      "Collected articles: 111/150\n",
      "Collected articles: 112/150\n",
      "Collected articles: 113/150\n",
      "Collected articles: 114/150\n",
      "Collected articles: 115/150\n",
      "Collected articles: 116/150\n",
      "Collected articles: 117/150\n",
      "Collected articles: 118/150\n",
      "Collected articles: 119/150\n",
      "Collected articles: 120/150\n",
      "Collected articles: 121/150\n",
      "Collected articles: 122/150\n",
      "Collected articles: 123/150\n",
      "Collected articles: 124/150\n",
      "Collected articles: 125/150\n",
      "Collected articles: 126/150\n",
      "Collected articles: 127/150\n",
      "Collected articles: 128/150\n",
      "Collected articles: 129/150\n",
      "Collected articles: 130/150\n",
      "Collected articles: 131/150\n",
      "Collected articles: 132/150\n",
      "Collected articles: 133/150\n",
      "Collected articles: 134/150\n",
      "Collected articles: 135/150\n",
      "Collected articles: 136/150\n",
      "Collected articles: 137/150\n",
      "Collected articles: 138/150\n",
      "Collected articles: 139/150\n",
      "Collected articles: 140/150\n",
      "Collected articles: 141/150\n",
      "Collected articles: 142/150\n",
      "Collected articles: 143/150\n",
      "Collected articles: 144/150\n",
      "Collected articles: 145/150\n",
      "Collected articles: 146/150\n",
      "Collected articles: 147/150\n",
      "Collected articles: 148/150\n",
      "Collected articles: 149/150\n",
      "Collected articles: 150/150\n"
     ]
    }
   ],
   "source": [
    "def scrape_wikipedia_article(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None  # Return None if the page request failed\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Finding the title of the article\n",
    "    title = soup.find('h1', id='firstHeading')  # Wikipedia uses this ID for article titles\n",
    "    article_title = title.get_text(strip=True) if title else 'Wikipedia Article'  # Fallback title if not found\n",
    "    \n",
    "    # Finding the main content of the article\n",
    "    content_div = soup.find('div', class_='mw-parser-output')\n",
    "    \n",
    "    # Extract text from all paragraphs within the content div, excluding certain elements\n",
    "    paragraphs = []\n",
    "    for p in content_div.find_all('p', recursive=True):\n",
    "        # Excluding paragraphs within tables or infoboxes\n",
    "        if not p.find_parent('table') and not p.find_parent('infobox'):\n",
    "            paragraphs.append(p.get_text(strip=True))\n",
    "    article_text = ' '.join(paragraphs)\n",
    "    \n",
    "    return {\n",
    "        'title': article_title,\n",
    "        'text': article_text.strip()\n",
    "    }\n",
    "\n",
    "# Initializing an empty list to hold the articles\n",
    "articles = []\n",
    "\n",
    "# base URL for random Wikipedia articles\n",
    "random_article_url = 'https://en.wikipedia.org/wiki/Special:Random'\n",
    "\n",
    "# number of articles we want to scrape\n",
    "target_article_count = 150\n",
    "\n",
    "while len(articles) < target_article_count:\n",
    "    # Scrape the article\n",
    "    article_data = scrape_wikipedia_article(random_article_url)\n",
    "    \n",
    "    # Check if the article was scraped successfully\n",
    "    if article_data and article_data['text']:\n",
    "        # Add the article to our list if it has content\n",
    "        articles.append({'url': random_article_url, 'title': article_data['title'], 'text': article_data['text'], 'label': 'Human-written'})\n",
    "        print(f\"Collected articles: {len(articles)}/{target_article_count}\")  # Progress output\n",
    "\n",
    "wiki_df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333c0d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     150 non-null    object\n",
      " 1   title   150 non-null    object\n",
      " 2   text    150 non-null    object\n",
      " 3   label   150 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "wiki_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f172cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wiki_df['text'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557f3a0",
   "metadata": {},
   "source": [
    "## Data storage for further analysis\n",
    "\n",
    "After successfully scraping and organizing the data, it is stored. This step allowed us to keep a stable and easily accessible dataset for further analysis, obviating the need to redo the scraping process. Opting for a pickle file as the storage medium was particularly advantageous due to its capacity to store Python objects, thereby maintaining the integrity of the data's structure and content. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd49abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df.to_pickle(\"wiki_data_300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856d31d",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Python Software Foundation. (n.d.). *Requests: HTTP for Humans™*. Retrieved from [https://requests.readthedocs.io](https://requests.readthedocs.io)\n",
    "\n",
    "- Richard Mitchell, Leonard Richardson. (n.d.). *Beautiful Soup Documentation*. Retrieved from [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "- Wes McKinney. (n.d.). *pandas: powerful Python data analysis toolkit*. Retrieved from [https://pandas.pydata.org/pandas-docs/stable/index.html](https://pandas.pydata.org/pandas-docs/stable/index.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
